{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRbuIIjtMMFj"
      },
      "source": [
        "# Homework 1 for CS 329P\n",
        "\n",
        "**Authors**: YOUR_NAMES\n",
        "\n",
        "**Emails**: YOUR_IDS@stanford.edu\n",
        "\n",
        "**Submission.** Please insert your names and emails above, save your code in this notebook, and explain what you are doing along with your findings in text cells. You can think of it as a technical report with code. Before submission, please use `Kernel -> Restart & Run All` in the Jupyter menu to verify your code is runnable and save all outputs. Afterwards, you can either upload your raw notebook (`hw1.ipynb`) or an exported PDF version to the `Homework 1` assignment in Canvas. \n",
        "\n",
        "\n",
        "In this homework, we will train a house sales price predictor on the data we scraped previously. The purpose of this homework is to let you practice different techniques that you can use to preprocess raw data. Your job is to obtain the best root mean squared logarithmic error (RMSLE) on the test dataset. To make your job easy, we provide sample code to train a model to report RMSLE and a list of ideas you can explore.\n",
        "\n",
        "**Note**: You can use either local runtimes to complete this assignment, or a hosted runtime (with GPU) on Colab. The second option generally runs faster. If using a local runtime, make sure that your Python version is less than 3.9 but at least 3.6, or you may have issues installing Autogluon. If using a runtime hosted on Colab, you can use the File Explorer pane on the left to upload the `house_sales.ftr` file. Make sure to wait until the file finishes uploading before running the next code block.\n",
        "\n",
        "Additionally, if using a local runtime, please refer to the [AG document](https://auto.gluon.ai/stable/index.html#installation) for info on how to install autogluon. \n",
        "\n",
        "## Prepare Data \n",
        "\n",
        "Let's first read in the dataset we used in our [Exploratory Data Analysis (EDA)](https://c.d2l.ai/stanford-cs329p/_static/notebooks/cs329p_notebook_eda.slides.html). Note that we use the [`feather` format](https://arrow.apache.org/docs/python/feather.html), which is faster to read than CSV but uses more disk space. The file `home_sales.ftr` can be downloaded from the Assignments folder in Canvas.\n",
        "\n",
        "Just for your information, it is generated with:\n",
        "\n",
        "```python\n",
        "data = pd.read_csv('house_sales.zip', dtype='unicode')\n",
        "data.to_feather('house_sales.ftr')\n",
        "```\n",
        "\n",
        "The following code needs at least 2GB memory. If using a local runtime, please make sure your machine has enough memory. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-22T21:19:22.818413Z",
          "start_time": "2021-09-22T21:19:18.899437Z"
        },
        "id": "YpsTjkMaMMFk"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_feather('house_sales.ftr')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mi5vrBMMKn-X"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "import numpy as np\n",
        "scipy.__version__, np.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlvsGd0AMMFl"
      },
      "source": [
        "We select a few common columns to make our training fast. You need to select more columns to make your model more accurate. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-22T21:19:22.837345Z",
          "start_time": "2021-09-22T21:19:22.821365Z"
        },
        "id": "vMIZGUkiMMFl"
      },
      "outputs": [],
      "source": [
        "df = data[['Sold Price', 'Sold On', 'Type', 'Year built', 'Bedrooms', 'Bathrooms']].copy()\n",
        "# uncomment the below line to save memory\n",
        "# del data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M025p8ZOMMFl"
      },
      "source": [
        "We copy the code from EDA to convert `Sold Price` to numerical values, which is our prediction target. We also remove examples whose prices are too high or too low."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-22T21:19:23.275937Z",
          "start_time": "2021-09-22T21:19:22.838990Z"
        },
        "id": "QEtKW8aZMMFl"
      },
      "outputs": [],
      "source": [
        "c = 'Sold Price'\n",
        "if c in df.select_dtypes('object').columns:\n",
        "    df.loc[:,c] = np.log10(\n",
        "            pd.to_numeric(df[c].replace(r'[$,-]', '', regex=True)) + 1)\n",
        "df = df[(df['Sold Price'] >= 4 ) & (df['Sold Price'] <= 8 )]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwS6XYWAMMFm"
      },
      "source": [
        "We use the house sales between 2021-2-15 and 2021-3-1 as our test data. You can use any example before 2021-2-15, but not after. In other words, we pretend we are launching our model on 2021-2-15 and testing it for 2 weeks. Here we only use sales in 2021 for fast training, but you can use more to improve accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-22T21:19:57.603573Z",
          "start_time": "2021-09-22T21:19:57.487636Z"
        },
        "id": "8HA0DTInMMFm"
      },
      "outputs": [],
      "source": [
        "test_start, test_end = pd.Timestamp(2021, 2, 15), pd.Timestamp(2021, 3, 1)\n",
        "train_start = pd.Timestamp(2021, 1, 1)\n",
        "df['Sold On'] = pd.to_datetime(df['Sold On'], errors='coerce')\n",
        "train = df[(df['Sold On'] >= train_start) & (df['Sold On'] < test_start)]\n",
        "test = df[(df['Sold On'] >= test_start) & (df['Sold On'] < test_end)]\n",
        "train.shape, test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH-Gx95nMMFn"
      },
      "source": [
        "Define our evaluation metric. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-22T21:19:58.917830Z",
          "start_time": "2021-09-22T21:19:58.915173Z"
        },
        "id": "FP0f3vqNMMFn"
      },
      "outputs": [],
      "source": [
        "def rmsle(y_hat, y):\n",
        "    # we already used log prices before, so we only need to compute RMSE\n",
        "    return sum((y_hat - y)**2 / len(y))**0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXFkQrUUMMFn"
      },
      "source": [
        "## AutoGluon Baseline\n",
        "\n",
        "We provide a baseline model trained by AutoGluon (AG). AG is an automl tool that performs automatic feature engineering, model selections, and ensemble. You are welcome to use any model and tool in achieving the best results possible in your homework. However, we recommend that you reuse the following training code so that you can focus on data preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-22T21:22:15.382886Z",
          "start_time": "2021-09-22T21:20:10.070746Z"
        },
        "id": "JPc8RXxJMMFn",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from autogluon.tabular import TabularPredictor\n",
        "    \n",
        "label = 'Sold Price'    \n",
        "predictor = TabularPredictor(label=label).fit(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpeILz5WMMFn"
      },
      "source": [
        "Test the performance of each model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-22T21:22:24.104096Z",
          "start_time": "2021-09-22T21:22:21.420090Z"
        },
        "id": "1as6hzTCMMFn"
      },
      "outputs": [],
      "source": [
        "predictor.leaderboard(test, silent=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1c0cM3QMMFo"
      },
      "source": [
        "Next, we compute the importance of each feature, along with several other metrics. It loooks like the `Sold On` feature is not very useful, likely because the houses in the test data were all sold late. You can choose to either remove such a feature, or find a way to extract a more useful presentation from it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-22T21:23:18.631587Z",
          "start_time": "2021-09-22T21:23:12.732269Z"
        },
        "id": "RM9BVHmeMMFo"
      },
      "outputs": [],
      "source": [
        "predictor.feature_importance(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiL2aZB9MMFo"
      },
      "source": [
        "Finally, let's predict and evaluate the RMSLE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-21T18:51:18.826116Z",
          "start_time": "2021-09-21T18:51:16.913391Z"
        },
        "id": "budOt2bGMMFo"
      },
      "outputs": [],
      "source": [
        "preds = predictor.predict(test.drop(columns=[label]))\n",
        "rmsle(preds, test[label])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-21T17:10:23.648923Z",
          "start_time": "2021-09-21T17:10:16.162130Z"
        },
        "id": "abfuZLOWMMFo"
      },
      "source": [
        "## Your Solution\n",
        "\n",
        "Please include your solution in the following section. (You are welcome to edit and delete code in previous sections).\n",
        "\n",
        "Your goal is to train a model using the features in the original dataset that minimizes the RMSLE on the validation dataset. While the naïve model achieves an RMSLE of ~0.3, it is possible to achieve an RMSLE of less than 0.08 on the same dataset.\n",
        "\n",
        "Here is a list of ideas you could explore:\n",
        "\n",
        "- More features: We only selected a small set of columns to use in training. You can add more, especially the ones we examined in EDA.\n",
        "- Data type conversion: Most data columns are strings; you may need to convert them into numerical values.\n",
        "- Data cleaning: There are NAN and outliers sprinkled throughout the dataset. You should find ways to selectively filter and remove them.\n",
        "- More examples: We only included sales made in 2021; there is a large number of examples in previous years that you can also include."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3yz5bBSMMFp"
      },
      "source": [
        "Check data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "data = pd.read_feather('house_sales.ftr')\n",
        "data['Type'].value_counts()[0:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df=data.copy()\n",
        "#按年份筛选\n",
        "df['Type']=df['Type'].replace('Single Family','SingleFamily')\n",
        "top_5_types = df['Type'].value_counts().index[:5]\n",
        "filtered_df = df[df['Type'].isin(top_5_types)]\n",
        "filtered_df=filtered_df[filtered_df['Year built']!='No Data']\n",
        "filtered_df['Year built'] = pd.to_numeric(filtered_df['Year built'], errors='coerce')\n",
        "filtered_df=filtered_df[filtered_df['Year built']>1800]\n",
        "#加热和制冷方式缺失太多直接drop掉\n",
        "filtered_df=filtered_df.drop('Heating',axis=1)\n",
        "filtered_df=filtered_df.drop('Cooling',axis=1)\n",
        "# 定义正则表达式模式，匹配Parking中包含 'Garage' 的类别\n",
        "pattern = r'.*Garage.*'\n",
        "# 将匹配的类别替换为 'Garage'\n",
        "filtered_df['Parking'] = filtered_df['Parking'].replace(to_replace=pattern, value='Garage', regex=True)\n",
        "filtered_df=filtered_df[filtered_df['Parking']!='No Data']\n",
        "filtered_df['Parking']=filtered_df['Parking'].replace('0 spaces','None')\n",
        "top_5_parking = filtered_df['Parking'].value_counts().index[:5]\n",
        "filtered_df = filtered_df[filtered_df['Parking'].isin(top_5_parking)]\n",
        "filtered_df['Parking']=filtered_df['Parking'].replace('None','No parking')\n",
        "pattern = r'.*Ground Floor Bedroom.*'\n",
        "filtered_df['Bedrooms'] = filtered_df['Bedrooms'].replace(to_replace=pattern, value='Ground Floor Bedroom', regex=True)\n",
        "filtered_df['Bedrooms']=filtered_df['Bedrooms'].replace('Ground Floor Bedroom','1')\n",
        "filtered_df = filtered_df.dropna(subset=['Bedrooms'])\n",
        "filtered_df=filtered_df[filtered_df['Bedrooms'].str.isdigit()]\n",
        "filtered_df['Bedrooms'] = pd.to_numeric(filtered_df['Bedrooms'], errors='coerce')\n",
        "final_df = filtered_df[['Sold Price', 'Sold On', 'Type', 'Year built', 'Bedrooms', 'Bathrooms']].copy()\n",
        "filtered_df['Bathrooms'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "c = 'Sold Price'\n",
        "if c in final_df.select_dtypes('object').columns:\n",
        "    final_df.loc[:,c] = np.log10(\n",
        "            pd.to_numeric(df[c].replace(r'[$,-]', '', regex=True)) + 1)\n",
        "final_df = final_df[(final_df['Sold Price'] >= 4 ) & (final_df['Sold Price'] <= 8 )]\n",
        "final_df = final_df.dropna(subset=['Bedrooms'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluation metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rmsle(y_hat, y):\n",
        "    # we already used log prices before, so we only need to compute RMSE\n",
        "    return sum((y_hat - y)**2 / len(y))**0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare for test dataset and train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_start, test_end = pd.Timestamp(2021, 2, 15), pd.Timestamp(2021, 3, 1)\n",
        "train_start = pd.Timestamp(2021, 1, 1)\n",
        "final_df['Sold On'] = pd.to_datetime(final_df['Sold On'], errors='coerce')\n",
        "train = final_df[(final_df['Sold On'] >= train_start) & (final_df['Sold On'] < test_start)]\n",
        "test = final_df[(final_df['Sold On'] >= test_start) & (final_df['Sold On'] < test_end)]\n",
        "train.shape, test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from autogluon.tabular import TabularPredictor\n",
        "    \n",
        "label = 'Sold Price'    \n",
        "predictor = TabularPredictor(label=label).fit(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictor.leaderboard(test, silent=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictor.feature_importance(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preds = predictor.predict(test.drop(columns=[label]))\n",
        "rmsle(preds, test[label])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "hw1_v2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
